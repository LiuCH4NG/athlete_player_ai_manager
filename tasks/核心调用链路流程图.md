# 核心调用链路流程图：后端 → 大模型 → MCP → 数据库

```mermaid
sequenceDiagram
    participant Client as 客户端请求
    participant Backend as FastAPI后端<br/>/chat/ 接口
    participant LLM as 大语言模型<br/>Ollama/OpenAI
    participant MCP as MCP服务器<br/>Model Context Protocol
    participant API as FastAPI路由<br/>/athletes/search
    participant CRUD as CRUD层<br/>crud.py
    participant DB as SQLite数据库<br/>athlete.db

    Note over Client,DB: 【核心调用链路】完整的AI查询流程

    %% 步骤1: 客户端发起请求
    Client->>Backend: GET /chat/?message=找一个北京的运动员

    %% 步骤2: 后端处理请求
    Backend->>Backend: 调用 run_agent(agent, message)
    activate Backend

    %% 步骤3: AI代理推理
    Backend->>LLM: agent.ainvoke({"messages": "找一个北京的运动员"})
    activate LLM
    Note right of LLM: 大模型分析用户意图<br/>• 意图: search<br/>• 参数: hometown="北京"
    LLM-->>Backend: 决策：需要调用search_athletes工具
    deactivate LLM

    %% 步骤4: 通过MCP调用工具
    Backend->>MCP: search_athletes({hometown: "北京"})
    activate MCP
    Note right of MCP: MCP客户端通过HTTP调用<br/>http://localhost:8001/mcp/

    %% 步骤5: MCP转发到FastAPI
    MCP->>API: HTTP POST /athletes/search/<br/>Body: {hometown: "北京"}
    activate API

    %% 步骤6: FastAPI路由处理
    API->>API: search_athletes(search_params, skip=0, limit=10)
    activate API

    %% 步骤7: 依赖注入获取数据库会话
    API->>API: get_db() [依赖注入]
    activate API
    Note right of API: 创建 AsyncSession 实例

    %% 步骤8: 调用CRUD层
    API->>CRUD: crud.search_athletes(db, search_params)
    activate CRUD

    %% 步骤9: 构建SQLAlchemy查询
    CRUD->>CRUD: 构建查询条件<br/>SELECT * FROM athletes<br/>WHERE is_deleted=false<br/>AND hometown ILIKE '%北京%'
    activate CRUD

    %% 步骤10: 执行数据库查询
    CRUD->>DB: EXECUTE SQL 查询
    activate DB
    Note right of DB: SQLite执行查询<br/>返回匹配的运动员记录

    DB-->>CRUD: 查询结果：[{athlete1}, {athlete2}, ...]
    deactivate DB

    CRUD-->>API: 返回运动员对象列表
    deactivate CRUD

    API-->>API: 转换数据格式<br/>Athlete -> AthleteInDB (Pydantic)
    deactivate API

    API-->>MCP: 返回athletes JSON数据
    deactivate API

    MCP-->>Backend: 返回工具调用结果
    deactivate MCP

    %% 步骤11: LLM格式化回复
    Backend->>LLM: llm.invoke(查询结果 + 原始问题)
    activate LLM
    Note right of LLM: 大模型将结构化数据<br/>转换为自然语言回复

    LLM-->>Backend: 生成最终回复<br/>"找到3位来自北京的运动员：..."
    deactivate LLM

    Backend-->>Backend: 返回 {"info": response}
    deactivate Backend

    Backend-->>Client: HTTP 200<br/>{"info": "找到3位来自北京的运动员：..."}
```

## 调用链路总结

### 1️⃣ **后端 (FastAPI)**
- **入口**: `/chat/` 接口
- **职责**: 创建AI代理、接收请求、返回响应
- **关键方法**: `create_agent()`, `run_agent()`

### 2️⃣ **大语言模型 (LLM)**
- **提供商**: Ollama (默认) 或 OpenAI
- **职责**:
  - 理解用户自然语言意图
  - 提取结构化查询参数
  - 格式化最终回复
- **推理链**: 理解 → 决策 → 工具选择 → 结果解释

### 3️⃣ **MCP服务器 (Model Context Protocol)**
- **协议**: streamable_http
- **端点**: http://localhost:8001/mcp/
- **职责**:
  - 封装API工具为LLM可用工具
  - 处理工具调用和参数传递
  - 转换数据格式

### 4️⃣ **数据库 (SQLite)**
- **ORM**: AsyncSQLAlchemy
- **职责**:
  - 持久化运动员数据
  - 执行查询条件
  - 返回结构化结果

## 关键数据流向

```
用户自然语言
    ↓
"找一个北京的运动员"
    ↓
LLM解析意图
    ↓
hometown="北京" (结构化参数)
    ↓
MCP工具调用
    ↓
/athletes/search API
    ↓
SQLAlchemy查询构建
    ↓
SELECT * FROM athletes WHERE hometown ILIKE '%北京%'
    ↓
SQLite执行
    ↓
[运动员记录列表]
    ↓
LLM格式化
    ↓
"找到3位来自北京的运动员：1. 张三 - 篮球运动员..."
```

## 技术要点

### LLM配置
- **Ollama**: `qwen3:4b` @ `http://127.0.0.1:11434`
- **OpenAI**: `gpt-3.5-turbo` @ `http://127.0.0.1:1234/v1`

### MCP工具映射
| LLM工具名 | 对应API端点 | 参数 |
|-----------|-------------|------|
| create_athlete | POST /athletes/ | AthleteCreate |
| get_athletes | GET /athletes/ | skip, limit |
| get_athlete | GET /athletes/{id} | athlete_id |
| update_athlete | PUT /athletes/{id} | athlete_id, AthleteUpdate |
| delete_athlete | DELETE /athletes/{id} | athlete_id |
| search_athletes | POST /athletes/search/ | AthleteSearch |

### 查询优化
- **软删除**: 所有查询自动添加 `is_deleted=false`
- **分页**: 默认 limit=10，避免大结果集
- **索引**: id, name 字段已建立索引
